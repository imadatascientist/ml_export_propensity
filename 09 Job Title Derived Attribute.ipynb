{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1a91c2-127a-4305-b437-c895f1e8ffce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatKSReport(ks_report):\n",
    "  ks_report.columns = ['decile', 'min', 'max', 'event', 'non_event', 'total', 'event_rate', 'cumulative_event', 'cumulative_non_event', 'ks', 'max_ks', 'l_cumulative_event', 'cumulative_total', 'l_cumulative_total', 'area', 'cumulative_area', 'area_b', 'pi', 'gini', 'auc']\n",
    "  ks_report['max_ks'] = np.where(ks_report['max_ks'] == '<----', '<', ks_report['max_ks'])\n",
    "  ks_report['decile'] = ks_report['decile'] + 5\n",
    "  ks_report['cumulative_event'] = ks_report['cumulative_event']/100\n",
    "  ks_report['cumulative_non_event'] = ks_report['cumulative_non_event']/100\n",
    "  ks_report['ks'] = ks_report['ks']/100\n",
    "  return ks_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da079124-5fac-4991-9763-6e28a562941b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_input = spark.read.parquet('/dbfs/FileStore/sahayk/frb_atl_sampling0_202203.parquet', header = False, inferSchema = True)\n",
    "\n",
    "df_gca = spark.sql('select duns_number as duns_gca from workarea.global_contact where load_year = 2022 and load_month = 2 and load_date = 20220220')\n",
    "\n",
    "df_gca = df_gca.dropDuplicates(['duns_gca'])\n",
    "\n",
    "df_wblinkage = spark.sql('select duns_nbr, hq_par_duns_nbr, stat_code, subs_code from workarea.wblinkage where load_year = 2022 and load_month = 2')\n",
    "\n",
    "df_wblinkage = df_wblinkage.withColumn('branch_location_indicator', when((col('stat_code') == 2) & (col('subs_code') == 0), 1).otherwise(0))\n",
    "\n",
    "df_wblinkage = df_wblinkage.withColumn('single_site_subsidiaries_indicator', when((col('stat_code') == 0) & (col('subs_code') == 3), 1).otherwise(0))\n",
    "\n",
    "df_gca2 = df_gca.join(df_wblinkage, col('duns_gca') == col('duns_nbr'), how = 'left')\n",
    "\n",
    "df_gca2 = df_gca2.withColumn('duns_gca2', when((col('branch_location_indicator') == 1) | (col('single_site_subsidiaries_indicator') == 1), col('hq_par_duns_nbr')).otherwise(col('duns_gca')))\n",
    "\n",
    "df_gca3 = df_gca2.dropDuplicates(['duns_gca2'])\n",
    "\n",
    "df_gca3 = df_gca3[['duns_gca2']]\n",
    "\n",
    "df_output = df_input.join(df_gca3, col('duns1') == col('duns_gca2'), how = 'inner')\n",
    "\n",
    "df_output.write.mode('overwrite').parquet('/dbfs/FileStore/sahayk/mark_seiss_files/frb_atl_sampling0_202203_output.parquet')\n",
    "\n",
    "df_output = spark.read.parquet('/dbfs/FileStore/sahayk/mark_seiss_files/frb_atl_sampling0_202203_output.parquet')\n",
    "\n",
    "df_output.write.mode('overwrite').saveAsTable('workarea.frb_atl_sampling_202203')\n",
    "\n",
    "gca = spark.sql('select duns_number as duns, jobtitle, gca_jobtitlefunctionnames, gca_jobtitlelevelnames, gca_primaryjobfunctionname from workarea.global_contact where load_year = 2020 and load_month = 3')\n",
    "gca.write.mode('overwrite').csv('/dbfs/FileStore/sahayk/us_export_propensity/us_export_propensity_job_title_data.csv')\n",
    "\n",
    "gca = spark.read.csv('/dbfs/FileStore/sahayk/us_export_propensity/us_export_propensity_job_title_data.csv', inferSchema = True, header = False)\\\n",
    ".withColumnRenamed('_c0', 'duns')\\\n",
    ".withColumnRenamed('_c1', 'job_title')\\\n",
    ".withColumnRenamed('_c2', 'job_function')\\\n",
    ".withColumnRenamed('_c3', 'job_title_level')\\\n",
    ".withColumnRenamed('_c4', 'primary_job_function')\n",
    "\n",
    "gca = gca.withColumn('match_ind', lit(1))\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(gca, on = ['duns'], how = 'left')\n",
    "train2 = train2.withColumn('match_ind', when(col('match_ind').isNull(), 0).otherwise(col('match_ind')))\n",
    "train2 = train2.withColumn('job_title_cleaned', ltrim(rtrim(lower(col('job_title')))))\n",
    "train2 = train2.withColumn('export_job_title_ind', when((col('job_title_cleaned').contains(' carrier')) |\n",
    "                                                   (col('job_title_cleaned').contains(' distribut')) |\n",
    "                                                   (col('job_title_cleaned').contains(' countri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' asia ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' global ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' deliveri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' trade'))\n",
    "                                                   , 1).otherwise(0))\n",
    "export_job_title = train2.groupBy('duns').agg(max('export_job_title_ind').alias('export_job_title_ind'))\n",
    "train3 = train.join(export_job_title, on = ['duns'], how = 'left')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(gca, on = ['duns'], how = 'left')\n",
    "val2 = val2.withColumn('match_ind', when(col('match_ind').isNull(), 0).otherwise(col('match_ind')))\n",
    "val2 = val2.withColumn('job_title_cleaned', ltrim(rtrim(lower(col('job_title')))))\n",
    "val2 = val2.withColumn('export_job_title_ind', when((col('job_title_cleaned').contains(' carrier')) |\n",
    "                                                   (col('job_title_cleaned').contains(' distribut')) |\n",
    "                                                   (col('job_title_cleaned').contains(' countri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' asia ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' global ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' deliveri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' trade'))\n",
    "                                                   , 1).otherwise(0))\n",
    "export_job_title = val2.groupBy('duns').agg(max('export_job_title_ind').alias('export_job_title_ind'))\n",
    "val3 = val.join(export_job_title, on = ['duns'], how = 'left')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(gca, on = ['duns'], how = 'left')\n",
    "test2 = test2.withColumn('match_ind', when(col('match_ind').isNull(), 0).otherwise(col('match_ind')))\n",
    "test2 = test2.withColumn('job_title_cleaned', ltrim(rtrim(lower(col('job_title')))))\n",
    "test2 = test2.withColumn('export_job_title_ind', when((col('job_title_cleaned').contains(' carrier')) |\n",
    "                                                   (col('job_title_cleaned').contains(' distribut')) |\n",
    "                                                   (col('job_title_cleaned').contains(' countri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' asia ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' global ')) |\n",
    "                                                   (col('job_title_cleaned').contains(' deliveri')) |\n",
    "                                                   (col('job_title_cleaned').contains(' trade'))\n",
    "                                                   , 1).otherwise(0))\n",
    "export_job_title = test2.groupBy('duns').agg(max('export_job_title_ind').alias('export_job_title_ind'))\n",
    "test3 = test.join(export_job_title, on = ['duns'], how = 'left')\n",
    "\n",
    "train3.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_job_title_derived_attribute')\n",
    "val3.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_job_title_derived_attribute')\n",
    "test3.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_job_title_derived_attribute')\n",
    "\n",
    "train4 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_job_title_derived_attribute')\n",
    "val4 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_job_title_derived_attribute')\n",
    "test4 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_job_title_derived_attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "val_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "test_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "train_original2 = train_original.join(train4, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val_original2 = val_original.join(val4, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "test_original2 = test_original.join(test4, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "train_original3 = train_original2.toPandas()\n",
    "val_original3 = val_original2.toPandas()\n",
    "test_original3 = test_original2.toPandas()\n",
    "train_original4 = train_original3[~train_original3['export_job_title_ind'].isna()]\n",
    "val_original4 = val_original3[~val_original3['export_job_title_ind'].isna()]\n",
    "test_original4 = test_original3[~test_original3['export_job_title_ind'].isna()]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train_original4[['export_job_title_ind']], train_original4[['export']])\n",
    "train_original4['predicted_export'] = clf.predict_proba(train_original4[['export_job_title_ind']])[:,1]\n",
    "val_original4['predicted_export'] = clf.predict_proba(val_original4[['export_job_title_ind']])[:,1]\n",
    "test_original4['predicted_export'] = clf.predict_proba(test_original4[['export_job_title_ind']])[:,1]\n",
    "train_original4['weight'] = 1\n",
    "val_original4['weight'] = 1\n",
    "test_original4['weight'] = 1\n",
    "\n",
    "ks = metrics(None, None, None, None, None, None)\n",
    "ks_report_train = ks.KS_train(train_original4['export'], train_original4['predicted_export'], train_original4['weight'], bins = 10)\n",
    "ks_report_val = ks.KS_train(val_original4['export'], val_original4['predicted_export'], val_original4['weight'], bins = 10)\n",
    "ks_report_test = ks.KS_train(test_original4['export'], test_original4['predicted_export'], test_original4['weight'], bins = 10)\n",
    "\n",
    "ks_report_train = formatKSReport(ks_report_train)\n",
    "ks_report_val = formatKSReport(ks_report_val)\n",
    "ks_report_test = formatKSReport(ks_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ks_report_train)\n",
    "\n",
    "display(ks_report_val)\n",
    "\n",
    "display(train4.groupBy('export_job_title_ind').agg(mean('export')))\n",
    "\n",
    "display(val4.groupBy('export_job_title_ind').agg(mean('export')))\n",
    "\n",
    "display(train4.groupBy('export_job_title_ind').count())\n",
    "\n",
    "display(val4.groupBy('export_job_title_ind').count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "09 Job Title Derived Attribute",
   "notebookOrigID": 1087800153333291,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
