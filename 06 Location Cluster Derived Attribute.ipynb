{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1a91c2-127a-4305-b437-c895f1e8ffce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfa78b5a-67c1-4a83-ae96-c5de8a1960bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "985c44b9-2bc5-4f17-a6d9-69e7d044efff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def posteriorProbability(df, control):\n",
    "  df = df.withColumn('control', lit(control))\n",
    "  df = df.withColumn('pre_alpha', col('state_sum')/col('control'))\n",
    "  df = df.withColumn('pre_beta', (col('state_size') - col('state_sum'))/col('control'))\n",
    "  df = df.withColumn('post_alpha', col('pre_alpha') + col('cluster_sum'))\n",
    "  df = df.withColumn('post_beta', col('pre_beta') + col('cluster_size') - col('cluster_sum'))\n",
    "  df = df.withColumn('posterior_probability', col('post_alpha')/(col('post_alpha') + col('post_beta')))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatKSReport(ks_report):\n",
    "  ks_report.columns = ['decile', 'min', 'max', 'event', 'non_event', 'total', 'event_rate', 'cumulative_event', 'cumulative_non_event', 'ks', 'max_ks', 'l_cumulative_event', 'cumulative_total', 'l_cumulative_total', 'area', 'cumulative_area', 'area_b', 'pi', 'gini', 'auc']\n",
    "  ks_report['max_ks'] = np.where(ks_report['max_ks'] == '<----', '<', ks_report['max_ks'])\n",
    "  ks_report['decile'] = ks_report['decile'] + 5\n",
    "  ks_report['cumulative_event'] = ks_report['cumulative_event']/100\n",
    "  ks_report['cumulative_non_event'] = ks_report['cumulative_non_event']/100\n",
    "  ks_report['ks'] = ks_report['ks']/100\n",
    "  return ks_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96ec0c0a-00e3-4f8e-b74b-677bf451ae24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smad = spark.sql('select duns_nbr as duns, alph_terr_code as state, geo_lat_sign, geo_lat_msmt, geo_long_sign, geo_long_msmt, cast(concat(geo_lat_sign, geo_lat_msmt) as int)/1000000 as latitude, cast(concat(geo_long_sign, geo_long_msmt) as int)/1000000 as longitude, load_year as append_year, load_month as append_month from workarea.smad where load_year in (2016, 2017) and load_month = 9 and geo_lat_sign = \"+\"')\n",
    "smad2 = smad[['duns', 'state', 'latitude', 'longitude', 'append_year', 'append_month']].dropDuplicates(['duns', 'append_year', 'append_month'])\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "train3 = train2.where('latitude <> 0 and longitude <> 0')\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols = [\"latitude\", \"longitude\"], outputCol = \"features\")\n",
    "\n",
    "train4 = vecAssembler.transform(train3)\n",
    "\n",
    "kmeans = KMeans(k = 500, seed = 1)\n",
    "model = kmeans.fit(train4.select('features'))\n",
    "\n",
    "model.save('/dbfs/FileStore/sahayk/us_export_propensity/pickles/us_export_propensity_analytic_dataset_train_cluster_model_k500_2.model')\n",
    "\n",
    "train5 = model.transform(train4) \n",
    "train6 = train5[['duns', 'append_year', 'append_month', 'export', 'state', 'latitude', 'longitude', 'prediction']]\n",
    "train6 = train6.withColumnRenamed('prediction', 'cluster_label')\n",
    "train6.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_location_cluster_k500')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "val3 = val2.where('latitude <> 0 and longitude <> 0')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "test3 = test2.where('latitude <> 0 and longitude <> 0')\n",
    "\n",
    "kmeans = KMeans()\n",
    "model = KMeansModel.load('/dbfs/FileStore/sahayk/us_export_propensity/pickles/us_export_propensity_analytic_dataset_train_cluster_model_k500_2.model')\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols = [\"latitude\", \"longitude\"], outputCol = \"features\")\n",
    "\n",
    "val4 = vecAssembler.transform(val3)\n",
    "val5 = model.transform(val4) \n",
    "val6 = val5[['duns', 'append_year', 'append_month', 'export', 'state', 'latitude', 'longitude', 'prediction']]\n",
    "val6 = val6.withColumnRenamed('prediction', 'cluster_label')\n",
    "val6.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_location_cluster_k500')\n",
    "\n",
    "test4 = vecAssembler.transform(test3)\n",
    "test5 = model.transform(test4)\n",
    "test6 = test5[['duns', 'append_year', 'append_month', 'export', 'state', 'latitude', 'longitude', 'prediction']]\n",
    "test6 = test6.withColumnRenamed('prediction', 'cluster_label')\n",
    "test6.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_location_cluster_k500')\n",
    "\n",
    "train7 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_location_cluster_k500')\n",
    "\n",
    "cluster_group = train7.groupBy(['cluster_label', 'state']).agg(sum('export').alias('cluster_sum'), count('duns').alias('cluster_size'))\n",
    "state_group = train7.groupBy(['state']).agg(sum('export').alias('state_sum'), count('duns').alias('state_size'))\n",
    "cluster_state_group = cluster_group.join(state_group, on = 'state', how = 'inner')\n",
    "cluster_state_group = cluster_state_group.withColumn('cluster_export_rate', col('cluster_sum')/col('cluster_size'))\n",
    "cluster_state_group = cluster_state_group.withColumn('state_export_rate', col('state_sum')/col('state_size'))\n",
    "\n",
    "loc_cluster_score1 = posteriorProbability(cluster_state_group, 100)\n",
    "loc_cluster_score1 = loc_cluster_score1.withColumnRenamed('posterior_probability', 'location_cluster_score1')\n",
    "\n",
    "loc_cluster_score1.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_location_cluster_score1_mapping_k500')\n",
    "\n",
    "train8 = train7.join(loc_cluster_score1[['cluster_label', 'state', 'location_cluster_score1']], on = ['cluster_label', 'state'], how = 'inner')\n",
    "train9 = train8\n",
    "\n",
    "train9.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_location_cluster_score_derived_attribute_k500')\n",
    "\n",
    "loc_cluster_score1 = spark.sql('select * from workarea.us_export_propensity_location_cluster_score1_mapping_k500')\n",
    "\n",
    "val7 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_location_cluster_k500')\n",
    "\n",
    "val8 = val7.join(loc_cluster_score1[['cluster_label', 'state', 'location_cluster_score1']], on = ['cluster_label', 'state'], how = 'inner')\n",
    "val9 = val8\n",
    "\n",
    "val9.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_location_cluster_score_derived_attribute_k500')\n",
    "\n",
    "test7 = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_location_cluster_k500')\n",
    "\n",
    "test8 = test7.join(loc_cluster_score1[['cluster_label', 'state', 'location_cluster_score1']], on = ['cluster_label', 'state'], how = 'inner')\n",
    "test9 = test8\n",
    "\n",
    "test9.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_location_cluster_score_derived_attribute_k500')\n",
    "\n",
    "train_df = spark.sql('select duns, location_cluster_score1, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_train_location_cluster_score_derived_attribute_k500')\n",
    "val_df = spark.sql('select duns, location_cluster_score1, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_val_location_cluster_score_derived_attribute_k500')\n",
    "\n",
    "train_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "val_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "\n",
    "train_original2 = train_original.join(train_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val_original2 = val_original.join(val_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train_original3 = train_original2.toPandas()\n",
    "val_original3 = val_original2.toPandas()\n",
    "\n",
    "train_original4 = train_original3[~train_original3['location_cluster_score1'].isna()]\n",
    "val_original4 = val_original3[~val_original3['location_cluster_score1'].isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train_original4[['location_cluster_score1']], train_original4[['export']])\n",
    "\n",
    "train_original4['predicted_export'] = clf.predict_proba(train_original4[['location_cluster_score1']])[:,1]\n",
    "val_original4['predicted_export'] = clf.predict_proba(val_original4[['location_cluster_score1']])[:,1]\n",
    "\n",
    "train_original4['weight'] = 1\n",
    "val_original4['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ae9375bd-7a2e-4aee-bf5e-57f51e26b628",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ks = metrics(None, None, None, None, None, None)\n",
    "ks_report_train = ks.KS_train(train_original4['export'], train_original4['predicted_export'], train_original4['weight'], bins = 10)\n",
    "ks_report_val = ks.KS_train(val_original4['export'], val_original4['predicted_export'], val_original4['weight'], bins = 10)\n",
    "ks_report_train = formatKSReport(ks_report_train)\n",
    "ks_report_val = formatKSReport(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dc3f7df-e068-4485-b3a2-3e189d19874d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "259cc087-b7b5-482d-ab12-7d46119840ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d1be022-154b-494b-b382-fbe1c7e2f6de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "06 Location Cluster Derived Attribute (k = 500)",
   "notebookOrigID": 2542747166870270,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
