{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05a21c29-7384-43de-8529-50ec9b39e149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "74b65256-44df-4c4d-9166-dbe49dd73a27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "628eda79-2d1c-44be-be7e-bb43feaa7b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14893b4a-5e80-483d-aaef-5e4e4fb91adb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def featureImportance(model, X):\n",
    "  feat_imp = pd.DataFrame(model.feature_importances_, index = X.columns)\n",
    "  feat_imp = feat_imp.reset_index()\n",
    "  feat_imp.columns = ['feature', 'feature_importance']\n",
    "  feat_imp = feat_imp.sort_values(by = 'feature_importance', ascending = False)\n",
    "  feat_imp['cumulative_feature_importance'] = feat_imp['feature_importance'].cumsum()\n",
    "\n",
    "  f = 'gain'\n",
    "  feat_gain = pd.DataFrame.from_dict(model.get_booster().get_score(importance_type = f), orient = 'index')\n",
    "  feat_gain = feat_gain.reset_index()\n",
    "  feat_gain.columns = ['feature', 'gain']\n",
    "\n",
    "  feat_imp = feat_imp.merge(feat_gain, on = 'feature', how = 'left')\n",
    "  feat_imp = feat_imp.sort_values('gain', ascending = False)\n",
    "  return feat_imp\n",
    "\n",
    "def formatKSReport(ks_report):\n",
    "  ks_report.columns = ['decile', 'min', 'max', 'event', 'non_event', 'total', 'event_rate', 'cumulative_event', 'cumulative_non_event', 'ks', 'max_ks', 'l_cumulative_event', 'cumulative_total', 'l_cumulative_total', 'area', 'cumulative_area', 'area_b', 'pi', 'gini', 'auc']\n",
    "  ks_report['max_ks'] = np.where(ks_report['max_ks'] == '<----', '<', ks_report['max_ks'])\n",
    "  ks_report['decile'] = ks_report['decile'] + 5\n",
    "  ks_report['cumulative_event'] = ks_report['cumulative_event']/100\n",
    "  ks_report['cumulative_non_event'] = ks_report['cumulative_non_event']/100\n",
    "  ks_report['ks'] = ks_report['ks']/100\n",
    "  return ks_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2abccb72-d493-4135-a58f-16573f9187a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/dbfs/FileStore/sahayk/us_export_propensity/input_data/us_export_propensity_data_prep_and_derivations.csv')\n",
    "\n",
    "loc_scr_train_df = spark.sql('select duns, location_cluster_score1 as location_cluster_score_k500, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_train_location_cluster_score_derived_attribute_k500')\n",
    "loc_scr_val_df = spark.sql('select duns, location_cluster_score1 as location_cluster_score_k500, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_val_location_cluster_score_derived_attribute_k500')\n",
    "loc_scr_test_df = spark.sql('select duns, location_cluster_score1 as location_cluster_score_k500, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_test_location_cluster_score_derived_attribute_k500')\n",
    "\n",
    "loc_scr_train_df = loc_scr_train_df.toPandas()\n",
    "loc_scr_val_df = loc_scr_val_df.toPandas()\n",
    "loc_scr_test_df = loc_scr_test_df.toPandas()\n",
    "\n",
    "trade_train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute')\n",
    "trade_val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute')\n",
    "trade_test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_importer_derived_attribute')\n",
    "\n",
    "trade_train_df = trade_train.toPandas()\n",
    "trade_val_df = trade_val.toPandas()\n",
    "trade_test_df = trade_test.toPandas()\n",
    "\n",
    "train_jobtitle = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_job_title_derived_attribute')\n",
    "val_jobtitle = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_job_title_derived_attribute')\n",
    "test_jobtitle = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_job_title_derived_attribute')\n",
    "\n",
    "train_jobtitle_df = train_jobtitle.toPandas()\n",
    "val_jobtitle_df = val_jobtitle.toPandas()\n",
    "test_jobtitle_df = test_jobtitle.toPandas()\n",
    "\n",
    "df['weight'] = 1\n",
    "df['ind_gctrs_3yrs'] = np.where(df['ind_gctrs_3yrs'] != 1, 0, df['ind_gctrs_3yrs'])\n",
    "\n",
    "train = df[df['sample_type'] == 'train']\n",
    "val = df[df['sample_type'] == 'val']\n",
    "test = df[df['sample_type'] == 'test']\n",
    "\n",
    "train = pd.merge(train, loc_scr_train_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val = pd.merge(val, loc_scr_val_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "test = pd.merge(test, loc_scr_test_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train = pd.merge(train, trade_train_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val = pd.merge(val, trade_val_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "test = pd.merge(test, trade_test_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train = pd.merge(train, train_jobtitle_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val = pd.merge(val, val_jobtitle_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "test = pd.merge(test, test_jobtitle_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train2 = train.copy()\n",
    "val2 = val.copy()\n",
    "test2 = test.copy()\n",
    "\n",
    "non_predictors = ['duns', 'export', 'sample_type', 'append_year', 'append_month', 'weight']\n",
    "predictors = [\n",
    "'gctrs_ttl_signal_3yrs',\n",
    "'ind_gctrs_3yrs',\n",
    "'gctrs_cnt_unq_yrs',\n",
    "'npayexp',\n",
    "'sic4_score',\n",
    "'nloc',\n",
    "'gctrs_cnt_unq_customer_country',\n",
    "'nrectyp',\n",
    "'satis',\n",
    "'sales',\n",
    "'drp_paydex1_loc_decile',\n",
    "'location_growth_score',\n",
    "'ncomptype',\n",
    "'nimptexpt',\n",
    "'foreign_trade_buyer_ind',\n",
    "'location_cluster_score',\n",
    "'miny_ownd_ind',\n",
    "'export_job_title_ind',\n",
    "'ba_sum_excl_12m',\n",
    "'loc_pct_rent_1',\n",
    "'sml_bus_ind',\n",
    "'loc_pct_comptype_g',\n",
    "'export_business_name_ind',\n",
    "'ucc_flng_3yr_cnt',\n",
    "'ba_count_info_src_12m',\n",
    "'chg_tot_emp',\n",
    "'inds_norm_pydx_scr',\n",
    "'drp_sales_loc_decile',\n",
    "]\n",
    "\n",
    "train2['drop_ind'] = np.where((train2['export'] == 1) & (train2['sic4_score'].isna()), 1, 0)\n",
    "val2['drop_ind'] = np.where((val2['export'] == 1) & (val2['sic4_score'].isna()), 1, 0)\n",
    "test2['drop_ind'] = np.where((test2['export'] == 1) & (test2['sic4_score'].isna()), 1, 0)\n",
    "\n",
    "train2 = train2[train2['drop_ind'] == 0]\n",
    "val2 = val2[val2['drop_ind'] == 0]\n",
    "test2 = test2[test2['drop_ind'] == 0]\n",
    "\n",
    "train2['export'].mean(), val2['export'].mean(), test2['export'].mean()\n",
    "\n",
    "train2 = train2.rename(columns = {'location_cluster_score_k500': 'location_cluster_score',\n",
    "                                 'exp_prop_bus_nme_ind': 'export_business_name_ind'})\n",
    "val2 = val2.rename(columns = {'location_cluster_score_k500': 'location_cluster_score',\n",
    "                                 'exp_prop_bus_nme_ind': 'export_business_name_ind'})\n",
    "test2 = test2.rename(columns = {'location_cluster_score_k500': 'location_cluster_score',\n",
    "                                 'exp_prop_bus_nme_ind': 'export_business_name_ind'})\n",
    "\n",
    "model = xgb.XGBClassifier(colsample_bylevel = 0.5, colsample_bytree = 0.6, learning_rate = 0.05, max_depth = 3, min_child_weight = 6, n_estimators = 400, subsample = 0.6)\n",
    "model.fit(train2[predictors], train2['export'], sample_weight = train2['weight'], eval_set = [(val2[predictors], val2['export'], val2['weight'])], eval_metric = 'auc', early_stopping_rounds = 8)\n",
    "\n",
    "pickle.dump(model, open('/dbfs/FileStore/sahayk/us_export_propensity/pickles/us_export_propensity_model_20220601.pkl', 'wb'))\n",
    "\n",
    "model = pickle.load(open('/dbfs/FileStore/sahayk/us_export_propensity/pickles/us_export_propensity_model_20220601.pkl', 'rb'))\n",
    "\n",
    "feature_list = list(model.get_booster().feature_names)\n",
    "\n",
    "train2['predicted_export'] = model.predict_proba(train2[feature_list], ntree_limit = model.best_ntree_limit)[:,1]\n",
    "val2['predicted_export'] = model.predict_proba(val2[feature_list], ntree_limit = model.best_ntree_limit)[:,1]\n",
    "test2['predicted_export'] = model.predict_proba(test2[feature_list], ntree_limit = model.best_ntree_limit)[:,1]\n",
    "\n",
    "feat_imp = featureImportance(model, train2[feature_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7160b217-780f-4172-b140-85df47e98c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65b77cdd-34c3-4139-ae7e-e51119901a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ks = metrics(None, None, None, None, None, None)\n",
    "ks_report_train = ks.KS_train(train2['export'], train2['predicted_export'], train2['weight'], bins = 10)\n",
    "ks_report_val = ks.KS_train(val2['export'], val2['predicted_export'], val2['weight'], bins = 10)\n",
    "ks_report_test = ks.KS_train(test2['export'], test2['predicted_export'], test2['weight'], bins = 10)\n",
    "\n",
    "ks_report_train = formatKSReport(ks_report_train)\n",
    "ks_report_val = formatKSReport(ks_report_val)\n",
    "ks_report_test = formatKSReport(ks_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4642b208-62f6-470c-9656-4fb42d8806fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e2785d6-3ef9-413a-b8ac-61afc79a7103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03975699-56ea-42ea-a16f-cbd53a4e8154",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30de0772-d3b7-48d0-90ab-9f06992a6d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_final = train2[non_predictors + feature_list + ['predicted_export']]\n",
    "val_final = val2[non_predictors + feature_list + ['predicted_export']]\n",
    "test_final = test2[non_predictors + feature_list + ['predicted_export']]\n",
    "\n",
    "pd.concat([train_final, val_final, test_final], ignore_index = True).to_csv('/dbfs/FileStore/sahayk/us_export_propensity/output_data/us_export_propensity_model_output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipment_train = spark.sql('select duns, append_year, append_month, export, non_us_shipment as foreign_trade_ind from workarea.us_export_propensity_analytic_dataset_train_shipping_data').dropDuplicates(['duns', 'append_year', 'append_month', 'export']).toPandas()\n",
    "shipment_val = spark.sql('select duns, append_year, append_month, export, non_us_shipment as foreign_trade_ind from workarea.us_export_propensity_analytic_dataset_val_shipping_data').dropDuplicates(['duns', 'append_year', 'append_month', 'export']).toPandas()\n",
    "shipment_test = spark.sql('select duns, append_year, append_month, export, non_us_shipment as foreign_trade_ind from workarea.us_export_propensity_analytic_dataset_test_shipping_data').dropDuplicates(['duns', 'append_year', 'append_month', 'export']).toPandas()\n",
    "\n",
    "train_final2  = pd.merge(train_final, shipment_train, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val_final2  = pd.merge(val_final, shipment_val, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "test_final2  = pd.merge(test_final, shipment_test, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train_final2['foreign_trade_ind'] = train_final2['foreign_trade_ind'].fillna(0)\n",
    "val_final2['foreign_trade_ind'] = val_final2['foreign_trade_ind'].fillna(0)\n",
    "test_final2['foreign_trade_ind'] = test_final2['foreign_trade_ind'].fillna(0)\n",
    "\n",
    "train_final2['predicted_export'] = np.where(train_final2['foreign_trade_ind'] == 1, 1, train_final2['predicted_export'])\n",
    "val_final2['predicted_export'] = np.where(val_final2['foreign_trade_ind'] == 1, 1, val_final2['predicted_export'])\n",
    "test_final2['predicted_export'] = np.where(test_final2['foreign_trade_ind'] == 1, 1, test_final2['predicted_export'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aaa0a03e-7acb-4b91-ba96-92e9a7c0d716",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ks2 = metrics(None, None, None, None, None, None)\n",
    "ks_report_train2 = ks2.KS_train(train_final2['export'], train_final2['predicted_export'], train_final2['weight'], bins = 10)\n",
    "ks_report_val2 = ks2.KS_train(val_final2['export'], val_final2['predicted_export'], val_final2['weight'], bins = 10)\n",
    "ks_report_test2 = ks2.KS_train(test_final2['export'], test_final2['predicted_export'], test_final2['weight'], bins = 10)\n",
    "\n",
    "ks_report_train2 = formatKSReport(ks_report_train2)\n",
    "ks_report_val2 = formatKSReport(ks_report_val2)\n",
    "ks_report_test2 = formatKSReport(ks_report_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4bfc535-6cb8-433b-b3f5-eb7ddab4d029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18f69881-b8f4-4a45-9b26-e890c3ce8daf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42b579c5-2908-4417-9478-e4dd46e4af06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf80d1e4-0edf-42ee-a0bc-a03cd8c35ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([train_final2, val_final2, test_final2], ignore_index = True).to_csv('/dbfs/FileStore/sahayk/us_export_propensity/output_data/us_export_propensity_model_output_override.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03 Model Scoring & Overrides",
   "notebookOrigID": 2257615712612022,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
