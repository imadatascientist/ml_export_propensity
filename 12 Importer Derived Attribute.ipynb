{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff93549e-e22c-4be2-babf-ee2fc6509d32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatKSReport(ks_report):\n",
    "  ks_report.columns = ['decile', 'min', 'max', 'event', 'non_event', 'total', 'event_rate', 'cumulative_event', 'cumulative_non_event', 'ks', 'max_ks', 'l_cumulative_event', 'cumulative_total', 'l_cumulative_total', 'area', 'cumulative_area', 'area_b', 'pi', 'gini', 'auc']\n",
    "  ks_report['max_ks'] = np.where(ks_report['max_ks'] == '<----', '<', ks_report['max_ks'])\n",
    "  ks_report['decile'] = ks_report['decile'] + 5\n",
    "  ks_report['cumulative_event'] = ks_report['cumulative_event']/100\n",
    "  ks_report['cumulative_non_event'] = ks_report['cumulative_non_event']/100\n",
    "  ks_report['ks'] = ks_report['ks']/100\n",
    "  return ks_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "63fa5336-79b4-4f1a-bf26-72693ed00fc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sic4_desc = spark.sql('select csic4 as sic4, sic4_description from workarea.sic4_desc')\n",
    "\n",
    "wblinkage = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(us_1987_sics_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wblinkage where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wbusunlinked = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(sic_base_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wbusunlinked where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wbglobalunlinked = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(us_1987_sics_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wbglobalunlinked where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wb = wblinkage.union(wbusunlinked).union(wbglobalunlinked)\n",
    "\n",
    "trade_2016 = spark.sql('select supp_duns_nbr as supplier_duns, rel_duns_nbr as buyer_duns, load_year, load_month from workarea.gt_purc_pstg_data_type3 where load_year in (2014, 2015, 2016)')\n",
    "\n",
    "trade_2016 = trade_2016.join(wb, (col('supplier_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_code', 'supplier_country_code')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_name', 'supplier_country_name')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_sic4', 'supplier_sic4')\n",
    "trade_2016 = trade_2016.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "trade_2016 = trade_2016.join(wb, (col('buyer_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_code', 'buyer_country_code')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_name', 'buyer_country_name')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_sic4', 'buyer_sic4')\n",
    "trade_2016 = trade_2016.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "\n",
    "trade_2016_timeperiod = trade_2016.groupBy(['load_year', 'load_month']).count().orderBy(['load_year', 'load_month'])\n",
    "trade_2016_timeperiod.registerTempTable('tb')\n",
    "trade_2016_timeperiod2 = spark.sql(\"\"\"\n",
    "select load_year, load_month, row_number() over (order by load_year desc, load_month desc) as id\n",
    "from tb \n",
    "\"\"\")\n",
    "trade_2016 = trade_2016.join(trade_2016_timeperiod2, on = ['load_year', 'load_month'], how = 'left')\n",
    "\n",
    "trade_2016 = trade_2016.where('id >= 4 AND id <= 28')\n",
    "\n",
    "trade_2016 = trade_2016.withColumn('buyer_append_year', lit(2016))\n",
    "trade_2016 = trade_2016.withColumn('buyer_append_month', lit(9))\n",
    "\n",
    "trade_2017 = spark.sql('select supp_duns_nbr as supplier_duns, rel_duns_nbr as buyer_duns, load_year, load_month from workarea.gt_purc_pstg_data_type3 where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "trade_2017 = trade_2017.join(wb, (col('supplier_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_code', 'supplier_country_code')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_name', 'supplier_country_name')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_sic4', 'supplier_sic4')\n",
    "trade_2017 = trade_2017.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "trade_2017 = trade_2017.join(wb, (col('buyer_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_code', 'buyer_country_code')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_name', 'buyer_country_name')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_sic4', 'buyer_sic4')\n",
    "trade_2017 = trade_2017.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "\n",
    "trade_2017_timeperiod = trade_2017.groupBy(['load_year', 'load_month']).count().orderBy(['load_year', 'load_month'])\n",
    "trade_2017_timeperiod.registerTempTable('tb')\n",
    "trade_2017_timeperiod2 = spark.sql(\"\"\"\n",
    "select load_year, load_month, row_number() over (order by load_year desc, load_month desc) as id\n",
    "from tb \n",
    "\"\"\")\n",
    "trade_2017 = trade_2017.join(trade_2017_timeperiod2, on = ['load_year', 'load_month'], how = 'left')\n",
    "trade_2017 = trade_2017.where('id >= 4 AND id <= 28')\n",
    "\n",
    "trade_2017 = trade_2017.withColumn('buyer_append_year', lit(2017))\n",
    "trade_2017 = trade_2017.withColumn('buyer_append_month', lit(9))\n",
    "\n",
    "trade = trade_2016.union(trade_2017)\n",
    "\n",
    "trade_final = trade[['supplier_duns', 'buyer_duns', 'supplier_country_code', 'supplier_country_name', 'supplier_sic4', 'buyer_country_code', 'buyer_country_name', 'buyer_sic4', 'buyer_append_year', 'buyer_append_month']]\n",
    "\n",
    "trade_final = trade_final.withColumn('trade_match_ind', lit(1))\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "\n",
    "train2 = train.join(trade_final, ((col('duns') == col('buyer_duns')) & (col('append_year') == col('buyer_append_year')) & (col('append_month') == col('buyer_append_month'))), how = 'left')\n",
    "\n",
    "train2.write.saveAsTable('workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute_trade_data')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(trade_final, ((col('duns') == col('buyer_duns')) & (col('append_year') == col('buyer_append_year')) & (col('append_month') == col('buyer_append_month'))), how = 'left')\n",
    "val2.write.saveAsTable('workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute_trade_data')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(trade_final, ((col('duns') == col('buyer_duns')) & (col('append_year') == col('buyer_append_year')) & (col('append_month') == col('buyer_append_month'))), how = 'left')\n",
    "test2.write.saveAsTable('workarea.us_export_propensity_analytic_dataset_test_importer_derived_attribute_trade_data')\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute_trade_data')\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute_trade_data')\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_importer_derived_attribute_trade_data')\n",
    "\n",
    "train = train.withColumn('foreign_trade_buyer_ind', lit(None))\n",
    "train = train.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') != 805, 1).otherwise(col('foreign_trade_buyer_ind')))\n",
    "train = train.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') == 805, 0).otherwise(col('foreign_trade_buyer_ind')))\n",
    "\n",
    "val = val.withColumn('foreign_trade_buyer_ind', lit(None))\n",
    "val = val.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') != 805, 1).otherwise(col('foreign_trade_buyer_ind')))\n",
    "val = val.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') == 805, 0).otherwise(col('foreign_trade_buyer_ind')))\n",
    "\n",
    "test = test.withColumn('foreign_trade_buyer_ind', lit(None))\n",
    "test = test.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') != 805, 1).otherwise(col('foreign_trade_buyer_ind')))\n",
    "test = test.withColumn('foreign_trade_buyer_ind', when(col('supplier_country_code') == 805, 0).otherwise(col('foreign_trade_buyer_ind')))\n",
    "\n",
    "train_grp = train.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_trade_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export','foreign_trade_buyer_ind'])\n",
    "val_grp = val.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_trade_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export','foreign_trade_buyer_ind'])\n",
    "test_grp = test.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_trade_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export','foreign_trade_buyer_ind'])\n",
    "\n",
    "train_grp2 = train_grp.groupBy(['duns','append_year', 'append_month', 'export']).agg(max('foreign_trade_buyer_ind').alias('foreign_trade_buyer_ind'))\n",
    "val_grp2 = val_grp.groupBy(['duns','append_year', 'append_month', 'export']).agg(max('foreign_trade_buyer_ind').alias('foreign_trade_buyer_ind'))\n",
    "test_grp2 = test_grp.groupBy(['duns','append_year', 'append_month', 'export']).agg(max('foreign_trade_buyer_ind').alias('foreign_trade_buyer_ind'))\n",
    "\n",
    "train_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute')\n",
    "val_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute')\n",
    "test_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_importer_derived_attribute')\n",
    "\n",
    "shipping = spark.sql('select shipper_duns, shipper_country_nme, shipper_sic4, consignee_duns, consignee_country_nme, consignee_sic4, est_arrival_tme from workarea.jzhang_shipments_all_cm_final6_temp1')\n",
    "\n",
    "shipping = shipping.withColumn('est_arrival_year', year(col('est_arrival_tme')))\n",
    "shipping = shipping.withColumn('est_arrival_month', month(col('est_arrival_tme')))\n",
    "\n",
    "shipping_2016 = shipping.where('est_arrival_year in (2015, 2016, 2017)')\n",
    "shipping_2017 = shipping.where('est_arrival_year in (2016, 2017, 2018)')\n",
    "\n",
    "shipping_2016 = shipping_2016.withColumn('consignee_append_year', lit(2016))\n",
    "shipping_2016 = shipping_2016.withColumn('consignee_append_month', lit(9))\n",
    "shipping_2017 = shipping_2017.withColumn('consignee_append_year', lit(2017))\n",
    "shipping_2017 = shipping_2017.withColumn('consignee_append_month', lit(9))\n",
    "\n",
    "shipping_final = shipping_2016.union(shipping_2017)\n",
    "\n",
    "shipping_final = shipping_final.withColumn('shipping_match_ind', lit(1))\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(shipping_final, (col('duns') == col('consignee_duns')) & (col('append_year') == col('consignee_append_year')) & (col('append_month') == col('consignee_append_month')), how = 'left')\n",
    "train3 = train2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "train3 = train3.drop('sic4')\n",
    "train3 = train3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "train4 = train3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "train4 = train4.drop('sic4')\n",
    "train4 = train4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "train4 = train4.withColumn('foreign_shipment_buyer_ind', lit(None))\n",
    "train4 = train4.withColumn('foreign_shipment_buyer_ind', when(~col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "train4 = train4.withColumn('foreign_shipment_buyer_ind', when(col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "\n",
    "train_grp = train4.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind'])\n",
    "train_grp2 = train_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('foreign_shipment_buyer_ind').alias('foreign_shipment_buyer_ind')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(shipping_final, (col('duns') == col('consignee_duns')) & (col('append_year') == col('consignee_append_year')) & (col('append_month') == col('consignee_append_month')), how = 'left')\n",
    "val3 = val2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "val3 = val3.drop('sic4')\n",
    "val3 = val3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "val4 = val3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "val4 = val4.drop('sic4')\n",
    "val4 = val4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "val4 = val4.withColumn('foreign_shipment_buyer_ind', lit(None))\n",
    "val4 = val4.withColumn('foreign_shipment_buyer_ind', when(~col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "val4 = val4.withColumn('foreign_shipment_buyer_ind', when(col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "\n",
    "val_grp = val4.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind'])\n",
    "val_grp2 = val_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('foreign_shipment_buyer_ind').alias('foreign_shipment_buyer_ind')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(shipping_final, (col('duns') == col('consignee_duns')) & (col('append_year') == col('consignee_append_year')) & (col('append_month') == col('consignee_append_month')), how = 'left')\n",
    "test3 = test2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "test3 = test3.drop('sic4')\n",
    "test3 = test3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "test4 = test3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "test4 = test4.drop('sic4')\n",
    "test4 = test4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "test4 = test4.withColumn('foreign_shipment_buyer_ind', lit(None))\n",
    "test4 = test4.withColumn('foreign_shipment_buyer_ind', when(~col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "test4 = test4.withColumn('foreign_shipment_buyer_ind', when(col('shipper_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('foreign_shipment_buyer_ind')))\n",
    "\n",
    "test_grp = test4.groupBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'foreign_shipment_buyer_ind'])\n",
    "test_grp2 = test_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('foreign_shipment_buyer_ind').alias('foreign_shipment_buyer_ind')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "train_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute2')\n",
    "val_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute2')\n",
    "test_grp2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_importer_derived_attribute2')\n",
    "\n",
    "trade_train=spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute')\n",
    "trade_val=spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_importer_derived_attribute')\n",
    "shipment=spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_importer_derived_attribute2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6f8757b-72b5-43de-b584-41c788c1b05d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "val_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "test_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "train_original2 = train_original.join(trade_train, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val_original2 = val_original.join(trade_val, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "train_original3 = train_original2.toPandas()\n",
    "val_original3 = val_original2.toPandas()\n",
    "train_original4 = train_original3[~train_original3['foreign_trade_buyer_ind'].isna()]\n",
    "val_original4 = val_original3[~val_original3['foreign_trade_buyer_ind'].isna()]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(train_original4[['foreign_trade_buyer_ind']], train_original4[['export']])\n",
    "train_original4['predicted_export'] = clf.predict_proba(train_original4[['foreign_trade_buyer_ind']])[:,1]\n",
    "val_original4['predicted_export'] = clf.predict_proba(val_original4[['foreign_trade_buyer_ind']])[:,1]\n",
    "train_original4['weight'] = 1\n",
    "val_original4['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "323405ff-6588-4236-8ce7-3f3e16c1d899",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ks = metrics(None, None, None, None, None, None)\n",
    "ks_report_train = ks.KS_train(train_original4['export'], train_original4['predicted_export'], train_original4['weight'], bins = 10)\n",
    "ks_report_val = ks.KS_train(val_original4['export'], val_original4['predicted_export'], val_original4['weight'], bins = 10)\n",
    "ks_report_train = formatKSReport(ks_report_train)\n",
    "ks_report_val = formatKSReport(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e95e02b6-f7e3-4398-a76e-5bf7a91dcf88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ddebc14b-c415-4a12-9688-359e225bb545",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_val)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "18 Importer Derived Attribute",
   "notebookOrigID": 2759888563019243,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
