{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1a91c2-127a-4305-b437-c895f1e8ffce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ddf483e7-1ece-448b-b152-46b3972c377a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sic4_desc = spark.sql('select csic4 as sic4, sic4_description from workarea.sic4_desc')\n",
    "\n",
    "wblinkage = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(us_1987_sics_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wblinkage where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wbusunlinked = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(sic_base_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wbusunlinked where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wbglobalunlinked = spark.sql('select duns_nbr as wb_duns, phy_ctry_code as wb_country_code, phy_ctry_nme as wb_country_name, cast(ltrim(rtrim(us_1987_sics_1)) as int) as wb_sic4, load_year as wb_load_year, load_month as wb_load_month from workarea.wbglobalunlinked where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "wb = wblinkage.union(wbusunlinked).union(wbglobalunlinked)\n",
    "\n",
    "trade_2016 = spark.sql('select supp_duns_nbr as supplier_duns, rel_duns_nbr as buyer_duns, load_year, load_month from workarea.gt_purc_pstg_data_type3 where load_year in (2014, 2015, 2016)')\n",
    "\n",
    "trade_2016 = trade_2016.join(wb, (col('supplier_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_code', 'supplier_country_code')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_name', 'supplier_country_name')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_sic4', 'supplier_sic4')\n",
    "trade_2016 = trade_2016.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "trade_2016 = trade_2016.join(wb, (col('buyer_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_code', 'buyer_country_code')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_country_name', 'buyer_country_name')\n",
    "trade_2016 = trade_2016.withColumnRenamed('wb_sic4', 'buyer_sic4')\n",
    "trade_2016 = trade_2016.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "\n",
    "trade_2016_timeperiod = trade_2016.groupBy(['load_year', 'load_month']).count().orderBy(['load_year', 'load_month'])\n",
    "trade_2016_timeperiod.registerTempTable('tb')\n",
    "trade_2016_timeperiod2 = spark.sql(\"\"\"\n",
    "select load_year, load_month, row_number() over (order by load_year desc, load_month desc) as id\n",
    "from tb \n",
    "\"\"\")\n",
    "trade_2016 = trade_2016.join(trade_2016_timeperiod2, on = ['load_year', 'load_month'], how = 'left')\n",
    "trade_2016 = trade_2016.where('id >= 4 AND id <= 28')\n",
    "\n",
    "trade_2016 = trade_2016.withColumn('supplier_append_year', lit(2016))\n",
    "trade_2016 = trade_2016.withColumn('supplier_append_month', lit(9))\n",
    "\n",
    "trade_2017 = spark.sql('select supp_duns_nbr as supplier_duns, rel_duns_nbr as buyer_duns, load_year, load_month from workarea.gt_purc_pstg_data_type3 where load_year in (2015, 2016, 2017)')\n",
    "\n",
    "trade_2017 = trade_2017.join(wb, (col('supplier_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_code', 'supplier_country_code')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_name', 'supplier_country_name')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_sic4', 'supplier_sic4')\n",
    "trade_2017 = trade_2017.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "trade_2017 = trade_2017.join(wb, (col('buyer_duns') == col('wb_duns')) & (col('load_year') == col('wb_load_year')) & (col('load_month') == col('wb_load_month')), how = 'left')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_code', 'buyer_country_code')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_country_name', 'buyer_country_name')\n",
    "trade_2017 = trade_2017.withColumnRenamed('wb_sic4', 'buyer_sic4')\n",
    "trade_2017 = trade_2017.drop(*['wb_duns', 'wb_load_year', 'wb_load_month'])\n",
    "\n",
    "trade_2017_timeperiod = trade_2017.groupBy(['load_year', 'load_month']).count().orderBy(['load_year', 'load_month'])\n",
    "trade_2017_timeperiod.registerTempTable('tb')\n",
    "trade_2017_timeperiod2 = spark.sql(\"\"\"\n",
    "select load_year, load_month, row_number() over (order by load_year desc, load_month desc) as id\n",
    "from tb \n",
    "\"\"\")\n",
    "trade_2017 = trade_2017.join(trade_2017_timeperiod2, on = ['load_year', 'load_month'], how = 'left')\n",
    "trade_2017 = trade_2017.where('id >= 4 AND id <= 28')\n",
    "\n",
    "trade_2017 = trade_2017.withColumn('supplier_append_year', lit(2017))\n",
    "trade_2017 = trade_2017.withColumn('supplier_append_month', lit(9))\n",
    "\n",
    "trade = trade_2016.union(trade_2017)\n",
    "\n",
    "trade_final = trade[['supplier_duns', 'buyer_duns', 'supplier_country_code', 'supplier_country_name', 'supplier_sic4', 'buyer_country_code', 'buyer_country_name', 'buyer_sic4', 'supplier_append_year', 'supplier_append_month']]\n",
    "\n",
    "trade_final = trade_final.withColumn('trade_match_ind', lit(1))\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(trade_final, ((col('duns') == col('supplier_duns')) & (col('append_year') == col('supplier_append_year')) & (col('append_month') == col('supplier_append_month'))), how = 'left')\n",
    "train2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_trade_data')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(trade_final, ((col('duns') == col('supplier_duns')) & (col('append_year') == col('supplier_append_year')) & (col('append_month') == col('supplier_append_month'))), how = 'left')\n",
    "val2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_trade_data')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(trade_final, ((col('duns') == col('supplier_duns')) & (col('append_year') == col('supplier_append_year')) & (col('append_month') == col('supplier_append_month'))), how = 'left')\n",
    "test2.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_trade_data')\n",
    "\n",
    "trade_data_train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_trade_data')\n",
    "trade_data_train = trade_data_train.withColumn('non_us_trade', lit(None))\n",
    "trade_data_train = trade_data_train.withColumn('non_us_trade', when(col('buyer_country_code') != 805, 1).otherwise(col('non_us_trade')))\n",
    "trade_data_train = trade_data_train.withColumn('non_us_trade', when(col('buyer_country_code') == 805, 0).otherwise(col('non_us_trade')))\n",
    "trade_data_train_grp = trade_data_train.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade'])\n",
    "trade_data_train_grp2 = trade_data_train_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_trade').alias('non_us_trade')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "trade_data_val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_trade_data')\n",
    "trade_data_val = trade_data_val.withColumn('non_us_trade', lit(None))\n",
    "trade_data_val = trade_data_val.withColumn('non_us_trade', when(col('buyer_country_code') != 805, 1).otherwise(col('non_us_trade')))\n",
    "trade_data_val = trade_data_val.withColumn('non_us_trade', when(col('buyer_country_code') == 805, 0).otherwise(col('non_us_trade')))\n",
    "trade_data_val_grp = trade_data_val.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade'])\n",
    "trade_data_val_grp2 = trade_data_val_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_trade').alias('non_us_trade')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "trade_data_test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_trade_data')\n",
    "trade_data_test = trade_data_test.withColumn('non_us_trade', lit(None))\n",
    "trade_data_test = trade_data_test.withColumn('non_us_trade', when(col('buyer_country_code') != 805, 1).otherwise(col('non_us_trade')))\n",
    "trade_data_test = trade_data_test.withColumn('non_us_trade', when(col('buyer_country_code') == 805, 0).otherwise(col('non_us_trade')))\n",
    "trade_data_test_grp = trade_data_test.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_trade'])\n",
    "trade_data_test_grp2 = trade_data_test_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_trade').alias('non_us_trade')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "shipping = spark.sql('select shipper_duns, shipper_country_nme, shipper_sic4, consignee_duns, consignee_country_nme, consignee_sic4, est_arrival_tme from workarea.jzhang_shipments_all_cm_final6_temp1')\n",
    "\n",
    "shipping = shipping.withColumn('est_arrival_year', year(col('est_arrival_tme')))\n",
    "shipping = shipping.withColumn('est_arrival_month', month(col('est_arrival_tme')))\n",
    "\n",
    "shipping_2016 = shipping.where('est_arrival_year in (2015, 2016, 2017)')\n",
    "shipping_2017 = shipping.where('est_arrival_year in (2016, 2017, 2018)')\n",
    "\n",
    "shipping_2016 = shipping_2016.withColumn('shipping_append_year', lit(2016))\n",
    "shipping_2016 = shipping_2016.withColumn('shipping_append_month', lit(9))\n",
    "shipping_2017 = shipping_2017.withColumn('shipping_append_year', lit(2017))\n",
    "shipping_2017 = shipping_2017.withColumn('shipping_append_month', lit(9))\n",
    "\n",
    "shipping_final = shipping_2016.union(shipping_2017)\n",
    "\n",
    "shipping_final = shipping_final.withColumn('shipping_match_ind', lit(1))\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(shipping_final, (col('duns') == col('shipper_duns')) & (col('append_year') == col('shipping_append_year')) & (col('append_month') == col('shipping_append_month')), how = 'left')\n",
    "\n",
    "train3 = train2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "train3 = train3.drop('sic4')\n",
    "train3 = train3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "train4 = train3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "train4 = train4.drop('sic4')\n",
    "train4 = train4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "\n",
    "train4 = train4.withColumn('non_us_shipment', lit(None))\n",
    "train4 = train4.withColumn('non_us_shipment', when(~col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('non_us_shipment')))\n",
    "train4 = train4.withColumn('non_us_shipment', when(col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('non_us_shipment')))\n",
    "\n",
    "train4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_shipping_data')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(shipping_final, (col('duns') == col('shipper_duns')) & (col('append_year') == col('shipping_append_year')) & (col('append_month') == col('shipping_append_month')), how = 'left')\n",
    "\n",
    "val3 = val2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "val3 = val3.drop('sic4')\n",
    "val3 = val3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "val4 = val3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "val4 = val4.drop('sic4')\n",
    "val4 = val4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "\n",
    "val4 = val4.withColumn('non_us_shipment', lit(None))\n",
    "val4 = val4.withColumn('non_us_shipment', when(~col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('non_us_shipment')))\n",
    "val4 = val4.withColumn('non_us_shipment', when(col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('non_us_shipment')))\n",
    "\n",
    "val4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_shipping_data')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(shipping_final, (col('duns') == col('shipper_duns')) & (col('append_year') == col('shipping_append_year')) & (col('append_month') == col('shipping_append_month')), how = 'left')\n",
    "\n",
    "test3 = test2.join(sic4_desc, col('shipper_sic4') == col('sic4'), how = 'left')\n",
    "test3 = test3.drop('sic4')\n",
    "test3 = test3.withColumnRenamed('sic4_description', 'shipper_sic4_description')\n",
    "test4 = test3.join(sic4_desc, col('consignee_sic4') == col('sic4'), how = 'left')\n",
    "test4 = test4.drop('sic4')\n",
    "test4 = test4.withColumnRenamed('sic4_description', 'consignee_sic4_description')\n",
    "\n",
    "test4 = test4.withColumn('non_us_shipment', lit(None))\n",
    "test4 = test4.withColumn('non_us_shipment', when(~col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 1).otherwise(col('non_us_shipment')))\n",
    "test4 = test4.withColumn('non_us_shipment', when(col('consignee_country_nme').isin([\"US\", \"UNITED STATES\"]), 0).otherwise(col('non_us_shipment')))\n",
    "\n",
    "test4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_shipping_data')\n",
    "\n",
    "shipping_data_train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train_shipping_data')\n",
    "shipping_data_train_grp = shipping_data_train.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment'])\n",
    "shipping_data_train_grp2 = shipping_data_train_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_shipment').alias('non_us_shipment')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "shipping_data_val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val_shipping_data')\n",
    "shipping_data_val_grp = shipping_data_val.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment'])\n",
    "shipping_data_val_grp2 = shipping_data_val_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_shipment').alias('non_us_shipment')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "shipping_data_test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test_shipping_data')\n",
    "shipping_data_test_grp = shipping_data_test.groupBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment']).count().orderBy(['duns', 'append_year', 'append_month', 'export', 'non_us_shipment'])\n",
    "shipping_data_test_grp2 = shipping_data_test_grp.groupBy(['duns', 'append_year', 'append_month', 'export']).agg(max('non_us_shipment').alias('non_us_shipment')).orderBy(['duns', 'append_year', 'append_month', 'export'])\n",
    "\n",
    "shipping_data_train_grp2 = shipping_data_train_grp2.withColumn('sample_type', lit('train'))\n",
    "shipping_data_val_train_grp2 = shipping_data_val_grp2.withColumn('sample_type', lit('val'))\n",
    "shipping_data_test_train_grp2 = shipping_data_test_grp2.withColumn('sample_type', lit('test'))\n",
    "shipping_data_final = shipping_data_train_grp2.union(shipping_data_val_train_grp2).union(shipping_data_test_train_grp2)\n",
    "shipping_data_final = shipping_data_final.withColumnRenamed('non_us_shipment', 'foreign_shipment_ind')\n",
    "shipping_data_final.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_foreign_shipment_derived_attribute')\n",
    "\n",
    "shipping_data_train_grp2.count(), shipping_data_val_grp2.count(), shipping_data_test_grp2.count()\n",
    "\n",
    "trade_data_train_grp2 = trade_data_train_grp2.withColumn('sample_type', lit('train'))\n",
    "trade_data_val_train_grp2 = trade_data_val_grp2.withColumn('sample_type', lit('val'))\n",
    "trade_data_test_train_grp2 = trade_data_test_grp2.withColumn('sample_type', lit('test'))\n",
    "trade_data_final = trade_data_train_grp2.union(trade_data_val_train_grp2).union(trade_data_test_train_grp2)\n",
    "trade_data_final = trade_data_final.withColumnRenamed('non_us_trade', 'foreign_trade_ind')\n",
    "trade_data_final.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_foreign_trade_derived_attribute')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "10 Trade & Shipping Derived Attribute",
   "notebookOrigID": 1087800153334094,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
