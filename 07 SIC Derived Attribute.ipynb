{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df1a91c2-127a-4305-b437-c895f1e8ffce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89ba2269-7dd6-4fe8-b6dc-86a0ae52e793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /users/sahayk/risk_ml_modeling_framework/model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatKSReport(ks_report):\n",
    "  ks_report.columns = ['decile', 'min', 'max', 'event', 'non_event', 'total', 'event_rate', 'cumulative_event', 'cumulative_non_event', 'ks', 'max_ks', 'l_cumulative_event', 'cumulative_total', 'l_cumulative_total', 'area', 'cumulative_area', 'area_b', 'pi', 'gini', 'auc']\n",
    "  ks_report['max_ks'] = np.where(ks_report['max_ks'] == '<----', '<', ks_report['max_ks'])\n",
    "  ks_report['decile'] = ks_report['decile'] + 5\n",
    "  ks_report['cumulative_event'] = ks_report['cumulative_event']/100\n",
    "  ks_report['cumulative_non_event'] = ks_report['cumulative_non_event']/100\n",
    "  ks_report['ks'] = ks_report['ks']/100\n",
    "  return ks_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "96ec0c0a-00e3-4f8e-b74b-677bf451ae24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smad = spark.sql('select duns_nbr as duns, prim_sic_code as sic4, load_year as append_year, load_month as append_month from workarea.smad where load_year in (2016, 2017) and load_month = 9')\n",
    "smad2 = smad[['duns', 'sic4', 'append_year', 'append_month']].dropDuplicates(['duns', 'append_year', 'append_month'])\n",
    "\n",
    "train = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "train2 = train.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "train3 = train2.where('sic4 > 0')\n",
    "\n",
    "val = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "val2 = val.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "val3 = val2.where('sic4 > 0')\n",
    "\n",
    "test = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_test')\n",
    "test2 = test.join(smad2, on = ['duns', 'append_year', 'append_month'], how = 'inner')\n",
    "test3 = test2.where('sic4 > 0')\n",
    "\n",
    "train3 = train3.withColumn('sic2', (train3.sic4.cast(IntegerType()) / 100).cast('int'))\n",
    "train3 = train3.withColumn('sic2d', lit(11))\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 0) & (col('sic2') <= 9), 1).otherwise(col('sic2d'))) # Agriculture, Forestry, Fishing\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 9) & (col('sic2') <= 14), 2).otherwise(col('sic2d'))) # Mining\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 14) & (col('sic2') <= 17), 3).otherwise(col('sic2d'))) # Construction\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 19) & (col('sic2') <= 39), 4).otherwise(col('sic2d'))) # Manufacturing\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 39) & (col('sic2') <= 49), 5).otherwise(col('sic2d'))) # Transportation & Public Utilities\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 49) & (col('sic2') <= 51), 6).otherwise(col('sic2d'))) # Wholesale Trade\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 51) & (col('sic2') <= 59), 7).otherwise(col('sic2d'))) # Retail Trade\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 59) & (col('sic2') <= 67), 8).otherwise(col('sic2d'))) # Finance, Insurance, Real Estate\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 69) & (col('sic2') <= 89), 9).otherwise(col('sic2d'))) # Services\n",
    "train3 = train3.withColumn('sic2d', when((col('sic2') > 90) & (col('sic2') <= 97), 0).otherwise(col('sic2d'))) # Public Administration\n",
    "\n",
    "val3 = val3.withColumn('sic2', (val3.sic4.cast(IntegerType()) / 100).cast('int'))\n",
    "val3 = val3.withColumn('sic2d', lit(11))\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 0) & (col('sic2') <= 9), 1).otherwise(col('sic2d'))) # Agriculture, Forestry, Fishing\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 9) & (col('sic2') <= 14), 2).otherwise(col('sic2d'))) # Mining\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 14) & (col('sic2') <= 17), 3).otherwise(col('sic2d'))) # Construction\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 19) & (col('sic2') <= 39), 4).otherwise(col('sic2d'))) # Manufacturing\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 39) & (col('sic2') <= 49), 5).otherwise(col('sic2d'))) # Transportation & Public Utilities\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 49) & (col('sic2') <= 51), 6).otherwise(col('sic2d'))) # Wholesale Trade\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 51) & (col('sic2') <= 59), 7).otherwise(col('sic2d'))) # Retail Trade\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 59) & (col('sic2') <= 67), 8).otherwise(col('sic2d'))) # Finance, Insurance, Real Estate\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 69) & (col('sic2') <= 89), 9).otherwise(col('sic2d'))) # Services\n",
    "val3 = val3.withColumn('sic2d', when((col('sic2') > 90) & (col('sic2') <= 97), 0).otherwise(col('sic2d'))) # Public Administration\n",
    "\n",
    "test3 = test3.withColumn('sic2', (test3.sic4.cast(IntegerType()) / 100).cast('int'))\n",
    "test3 = test3.withColumn('sic2d', lit(11))\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 0) & (col('sic2') <= 9), 1).otherwise(col('sic2d'))) # Agriculture, Forestry, Fishing\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 9) & (col('sic2') <= 14), 2).otherwise(col('sic2d'))) # Mining\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 14) & (col('sic2') <= 17), 3).otherwise(col('sic2d'))) # Construction\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 19) & (col('sic2') <= 39), 4).otherwise(col('sic2d'))) # Manufacturing\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 39) & (col('sic2') <= 49), 5).otherwise(col('sic2d'))) # Transportation & Public Utilities\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 49) & (col('sic2') <= 51), 6).otherwise(col('sic2d'))) # Wholesale Trade\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 51) & (col('sic2') <= 59), 7).otherwise(col('sic2d'))) # Retail Trade\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 59) & (col('sic2') <= 67), 8).otherwise(col('sic2d'))) # Finance, Insurance, Real Estate\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 69) & (col('sic2') <= 89), 9).otherwise(col('sic2d'))) # Services\n",
    "test3 = test3.withColumn('sic2d', when((col('sic2') > 90) & (col('sic2') <= 97), 0).otherwise(col('sic2d'))) # Public Administration\n",
    "\n",
    "sic2_group = train3.groupBy('sic2').agg(sum('export').alias('sic2_sum'), count('duns').alias('sic2_count'))\n",
    "\n",
    "sic2d_group = train3.groupBy('sic2d').agg(sum('export').alias('sic2d_sum'), count('duns').alias('sic2d_count'))\n",
    "\n",
    "sic4_group = train3.groupBy(['sic4', 'sic2', 'sic2d']).agg(sum('export').alias('sic4_sum'), count('duns').alias('sic4_count'))\n",
    "\n",
    "sic4_sic2_group = sic4_group.join(sic2_group, on = ['sic2'], how = 'inner')\n",
    "\n",
    "sic4_sic2_sic2d_group = sic4_sic2_group.join(sic2d_group, on = ['sic2d'], how = 'inner')\n",
    "\n",
    "sic4_sic2_sic2d_group = sic4_sic2_sic2d_group.withColumn('sic4_export_rate', col('sic4_sum')/col('sic4_count'))\n",
    "sic4_sic2_sic2d_group = sic4_sic2_sic2d_group.withColumn('sic2_export_rate', col('sic2_sum')/col('sic2_count'))\n",
    "sic4_sic2_sic2d_group = sic4_sic2_sic2d_group.withColumn('sic2d_export_rate', col('sic2d_sum')/col('sic2d_count'))\n",
    "\n",
    "df = sic4_sic2_sic2d_group\n",
    "\n",
    "df = df.withColumn('control', lit(100))\n",
    "\n",
    "df = df.withColumn('pre_alpha0', col('sic2d_sum')/col('control'))\n",
    "df = df.withColumn('pre_beta0', (col('sic2d_count')/col('control')) + (-(col('sic2d_sum')/col('control'))))\n",
    "\n",
    "df = df.withColumn('pre_alpha', (col('sic2_sum') + col('pre_alpha0'))/col('control'))\n",
    "df = df.withColumn('pre_beta', (col('pre_beta0') + (col('sic2_count') - col('sic2_sum')))/col('control'))\n",
    "\n",
    "df = df.withColumn('post_alpha', col('pre_alpha') + col('sic4_sum'))\n",
    "df = df.withColumn('post_beta', col('pre_beta') + col('sic4_count') - col('sic4_sum'))\n",
    "\n",
    "df = df.withColumn('posterior_probability', col('post_alpha')/(col('post_alpha') + col('post_beta')))\n",
    "\n",
    "sic4_score = df[['sic4', 'sic2', 'sic2d', 'posterior_probability']]\n",
    "sic4_score = sic4_score.withColumnRenamed('posterior_probability', 'sic4_score')\n",
    "\n",
    "train4 = train3.join(sic4_score, on = ['sic4', 'sic2', 'sic2d'], how = 'inner')\n",
    "\n",
    "train4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_train_sic4_score_derived_attribute')\n",
    "\n",
    "df = df.withColumnRenamed('posterior_probability', 'sic4_score')\n",
    "\n",
    "df.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_sic4_score_mapping')\n",
    "\n",
    "sic4_score = spark.sql('select * from workarea.us_export_propensity_sic4_score_mapping')\n",
    "\n",
    "val4 = val3.join(sic4_score[['sic4', 'sic2', 'sic2d', 'sic4_score']], on = ['sic4', 'sic2', 'sic2d'], how = 'inner')\n",
    "\n",
    "test4 = test3.join(sic4_score[['sic4', 'sic2', 'sic2d', 'sic4_score']], on = ['sic4', 'sic2', 'sic2d'], how = 'inner')\n",
    "\n",
    "val4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_val_sic4_score_derived_attribute')\n",
    "\n",
    "test4.write.mode('overwrite').saveAsTable('workarea.us_export_propensity_analytic_dataset_test_sic4_score_derived_attribute')\n",
    "\n",
    "train_df = spark.sql('select duns, sic4_score, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_train_sic4_score_derived_attribute')\n",
    "val_df = spark.sql('select duns, sic4_score, append_year, append_month, export from workarea.us_export_propensity_analytic_dataset_val_sic4_score_derived_attribute')\n",
    "\n",
    "train_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_train')\n",
    "val_original = spark.sql('select * from workarea.us_export_propensity_analytic_dataset_val')\n",
    "\n",
    "train_original2 = train_original.join(train_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "val_original2 = val_original.join(val_df, on = ['duns', 'append_year', 'append_month', 'export'], how = 'left')\n",
    "\n",
    "train_original3 = train_original2.toPandas()\n",
    "val_original3 = val_original2.toPandas()\n",
    "\n",
    "train_original4 = train_original3[~train_original3['sic4_score'].isna()]\n",
    "val_original4 = val_original3[~val_original3['sic4_score'].isna()]\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(train_original4[['sic4_score']], train_original4[['export']])\n",
    "\n",
    "train_original4['predicted_export'] = clf.predict_proba(train_original4[['sic4_score']])[:,1]\n",
    "val_original4['predicted_export'] = clf.predict_proba(val_original4[['sic4_score']])[:,1]\n",
    "\n",
    "train_original4['weight'] = 1\n",
    "val_original4['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec4eac1e-ee54-4f9e-906b-bd6a04b2fcfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ks = metrics(None, None, None, None, None, None)\n",
    "ks_report_train = ks.KS_train(train_original4['export'], train_original4['predicted_export'], train_original4['weight'], bins = 10)\n",
    "ks_report_val = ks.KS_train(val_original4['export'], val_original4['predicted_export'], val_original4['weight'], bins = 10)\n",
    "ks_report_train = formatKSReport(ks_report_train)\n",
    "ks_report_val = formatKSReport(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6404d52-3a32-454c-80bc-62dfb1b08c51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "623aaab5-2500-4d6f-abb9-f12feb71e994",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(ks_report_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3590696-bb33-44e4-8720-416007cbcff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "07 SIC Derived Attribute",
   "notebookOrigID": 1087800153315453,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
